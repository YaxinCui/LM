# 语言模型学习

- [ ] LSTM-->Attention-->Seq2Seq-->Transformer-->Elmo-->Bert-->GPT-->XLnet-->Ernie

- [ ] 看已经发表的综述

>看完内容后再做笔记，最好有自己的思考
>
>记录经典论文
>
>看看有没有具体实验
>
>最好看完后就写那部分综述

---

### Attention

>  https://easyai.tech/ai-definition/attention/ 

1. Attention的本质是什么？
   1. 加权求和
2. Attention的3大优点
   1. 参数少
   2. 速度快
   3. 效果好
3. Attention的原理
   1. query和key进行相似度计算，得到权值
   2. 将权值进行归一化，得到直接可用的权重
   3. 将权重和value进行加权求和
4. Attention的N种类型
   1. 根据计算区域划分
      1. Soft Attention
      2. Hard Attention
      3. Local Attention
   2. 所用信息
      1. General Attention
      2.  Local Attention
   3. 结构层次
      1. 单层Attention
      2. 多层Attention
      3. 多头Attention
   4. 模型方面
      1. CNN+Attention
      2. LSTM+Attention
      3. 纯Attention

?key跟query是什么，又是怎么来的，有什么用？

> 深度学习中的注意力机制 https://blog.csdn.net/tg229dvt5i93mxaq5a6u/article/details/78422216 

？集成模型，生成同一个中间编码，使用不同任务的decoder+encoder来训练它

？

>  https://zhuanlan.zhihu.com/p/77307258 



> Attention原理和源码解析 https://zhuanlan.zhihu.com/p/43493999 

离得近的两个词更密切，位置编码表达出了相关性。

2. 语言模型的定义和BERT解读。

语言模型：求P(c1,……,cm)里面c1到cm出现的概率

Bert：Bidirectional Encoder Representtations from Transformers。双向transformer编码表达。

E1是指单个字，训练过程以字为最小单位。



在BERT中，主要以两种方式。

MASKED LM，随机遮盖或替换一句话。

1. 把一句话15%的token替换成以下内
   1. 这些token有80%的几率被替换成[mask]
   2. 有10%的几率替换成任意一个其他的tokern；
   3. 有10的概率几率原封不动。

2.之后让模型预测和还原被遮盖掉的或替换掉的部分。

BERT语言模型任务二：Next Sentence Prediction

1. 首先我们拿属于上下文的一对句子，在两端连续的句子里面加一些特殊toke。

用维基百科的数据，



BERT base，12 transformer_block。1.1亿参数，3.4亿参数。1周

1. 按字为单位训练BERT，在Masked LM里面，把句子中的英文单词分出来，将英文单词所在区域一起遮盖掉，让模型预测这个部分。
2. 很多句子里含有数字，在Masked LM中，让模型准确预测数据是不现实的，把原文中的数字都替换成一特殊toen,#NUM#，这样模型只要预测出这个地方应该是某些数字就可以了。

5. 使用BERT预训练模型进行自然语言的情感分类
6. 1. 情感分析语料预处理，
   2. 

1. 平均长度
2. 把所有的语料放在list里面，每一条语料是一个dict。
3. 

BERT

sklearn可以引用AUC。

在维基百科的基础上做微调。

mean_max_pooling。

比赛中修改bert的一些方法：

1. L2正则化
2. 调整trannsformer block个数
3. 使用dropout

写一个脚本，用来寻找最大的阈值边界。

只取出CLS版本，数据增强











---

> bilibili，LSTM长短期记忆神经网络的学习与实现

2. Recurrent Neural Networks
3. Standard RNN Shortcomings
4. Long Short Term Memory
5. Applications of LSTM Networks









---

> bilibili，从中文Transformer到BERT的模型精讲 https://www.bilibili.com/video/av73631845?from=search&seid=3819548450683704648 

Bert是Transformer的一种，是一种预训练模型。

集成学习。

!论文Attention is all you need

上游任务（预训练语言模型） + 下游任务（具体任务）

充分理解transformer并具备一定衍生模型的设计和编写能力。

---

一.transformer编码器（理论部分）

0. transformer模型的直觉，建立直观认识；
1. positional encoding，即位置嵌入（或位置编码）
2. self attention mechanism，即自注意力机制与注意力矩阵可视化
3. Layer Normalization和残差连接
4. transformer encoder整体结构

二.transformer代码解读，语料数据预处理，BERT的预训练和情感分析的应用

三.sequence 2 sequence（序列到序列）模型或Name Entity Recognition（命名实体识别）

---

transformer的训练是并行的，transformer使用了位置嵌入（positional encoding）来理解语言的顺序，使用自注意力机制和全连接层来进行计算。

编码器+解码器

![image-20191125145521025](%E4%BB%8E%E4%B8%AD%E6%96%87Transformer%E5%88%B0BERT%E7%9A%84%E6%A8%A1%E5%9E%8B%E7%B2%BE%E8%AE%B2.assets/image-20191125145521025.png)

？为什么transformer是并行的，但要把上一个解码器输出输入到下一个编码器输入中。

Encoding->编码器：把自然语言序列经过计算到隐藏层的过程。

X~embedding~[batch size, sequence length, embedding dimension]

Positional encoding，位置嵌入，位置嵌入的维度为[max sequence length, embedding dimension]，嵌入的维度同词向量的维度，max sequence lenght属于超参数，指的是限定的最大单个句长。

一般以字为单位训练transformer模型，也就是说不用分词了，首先我们要初始化字向量为[vocab size, embedding dimension], vocab size为总共的字库数量，在这里论文中使用sine和cosine函数的线性变换来提供给模型信息

PE~(pos,2i)~=sin(pos/10000^2i/d_model^)

PE(pos,2i+1)=cos(pos/10000^2i/d_model^)

上式中pos指的是句中字的位置，取值范围是[0, max sequence length], i指的是词向量的维度，取值范围是[0, embedding dimension]，每一个位置在embedding dimension维度上都会得到不同周期的sin和cos函数的取值组合，从而获得独一的纹理位置信息，模型从而学到位置之间的依赖关系和自然语言的时序特性。

### 综述

#### 摘要

语言模型是对语言建模的重要方法，对理解与利用自然语言有重要的作用。自从语言模型与神经网络相结合，语言模型得到了飞速的发展，在性能上，取得了十分大的进步，在多项任务中效果得到了显著的提升。神经网络语言模型的发展与历程，值得归纳总结。本文将介绍2种经典的浅层词向量模型：NNLM模型与RNNLM模型，还将介绍ELMo、GPT、BERT等几种代表性深层次表示模型及模型的学习方法。



#### NNLM



### Attention



### Transformer



### Elmo

> ELMo原理解析及简单上手使用，https://zhuanlan.zhihu.com/p/51679783

用浅层词向量用一个词对应一个词向量，现实中经常会出现一词多义的情况，比如bank有银行的意思，也有河岸的意思，随着语言的演变，还可能有新的意思，这是急需解决的问题。Elmo以机器翻译为下游任务，演示了biLM的

